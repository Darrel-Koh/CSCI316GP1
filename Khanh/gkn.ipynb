{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import lib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data.csv')\n",
    "df = pd.read_csv('../data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null\n",
    "null_counts = df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover and visualise the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove irrelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_col = [\n",
    "    'id',\n",
    "    'member_id',\n",
    "    'emp_title',\n",
    "    'issue_d',\n",
    "    'desc',\n",
    "    'title',\n",
    "    'zip_code',\n",
    "    'addr_state',\n",
    "    'earliest_cr_line',\n",
    "    'last_pymnt_d',\n",
    "    'last_pymnt_amnt',\n",
    "    'next_pymnt_d',\n",
    "    'last_credit_pull_d',\n",
    "    'collections_12_mths_ex_med',\n",
    "    'mths_since_last_major_derog',\n",
    "    'policy_code',\n",
    "\n",
    "    # Vu Anh\n",
    "    # 'home-ownership'\n",
    "    # 'purpose', \n",
    "    # 'sub_grade',\n",
    "\n",
    "    'tot_coll_amt', # This can be removed \n",
    "    'tot_cur_bal', \n",
    "    'open_acc_6m', # Remove because all null value got defautl = 1\n",
    "    'open_il_6m', \n",
    "\n",
    "    # Darrel\n",
    "    # 'open_il_12m', \n",
    "    # 'open_il_24m', \n",
    "    # 'mths_since_rcnt_il', \n",
    "    # 'total_bal_il', \n",
    "    # 'il_util', \n",
    "    # 'open_rv_12m' ,\n",
    "    # 'open_rv_24m', \n",
    "\n",
    "    # Vanness\n",
    "    'max_bal_bc', \n",
    "    'all_util', \n",
    "    'total_rev_hi_lim', \n",
    "    'inq_fi', \n",
    "    'total_cu_tl', \n",
    "    'inq_last_12m'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion of default = 1 and default = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts of 'default_ind'\n",
    "value_counts = df['default_ind'].value_counts()\n",
    "\n",
    "# Calculate the proportions\n",
    "proportion_0 = value_counts[0] / len(df)\n",
    "proportion_1 = value_counts[1] / len(df)\n",
    "\n",
    "print(f\"Proportion of 0: {proportion_0 * len(df)}\")\n",
    "print(f\"Proportion of 1: {proportion_1 * len(df)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle home-ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with value ANY\n",
    "\n",
    "df = df[df['home_ownership'] != 'ANY']\n",
    "\n",
    "df['home_ownership'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level encoding for home ownership \n",
    "\n",
    "home_type = ['RENT', 'OWN', 'MORTGAGE', 'OTHER', 'NONE']  # Unique values for encoding\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(home_type)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['home_ownership'] = encoder.transform(df['home_ownership'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['purpose'].unique()\n",
    "\n",
    "purposes = ['credit_card', 'car', 'small_business', 'other', 'wedding',\n",
    "       'debt_consolidation', 'home_improvement', 'major_purchase',\n",
    "       'medical', 'moving', 'vacation', 'house', 'renewable_energy',\n",
    "       'educational']\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(purposes)\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['purpose'] = encoder.transform(df['purpose'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each label\n",
    "label_counts = df['purpose'].value_counts().sort_index()\n",
    "\n",
    "# Print the label counts\n",
    "\n",
    "'''\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"{label}: {count}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle sub_grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sub-grade is a more detailed classification that further divides borrowers within each grade. It typically includes additional factors such as credit history, employment stability, and debt-to-income ratio. This additional level of detail can help in distinguishing the risk profile of borrowers within the same grade.\n",
    "\n",
    "This is why we should keep the subgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sub_grade'].unique()\n",
    "\n",
    "# TODO: check if we should use data binning for this attribute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make best use of level encoding, we sort the sub-grade by order to present the relationship between subgrades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgrades = ['B2', 'C4', 'C5', 'C1', 'B5', 'A4', 'E1', 'F2', 'C3', 'B1', 'D1',\n",
    "       'A1', 'B3', 'B4', 'C2', 'D2', 'A3', 'A5', 'D5', 'A2', 'E4', 'D3',\n",
    "       'D4', 'F3', 'E3', 'F4', 'F1', 'E5', 'G4', 'E2', 'G3', 'G2', 'G1',\n",
    "       'F5', 'G5']\n",
    "\n",
    "def custom_sort_key(subgrade):\n",
    "    match = re.match(r'([A-Za-z]+)(\\d+)', subgrade)\n",
    "    letter = match.group(1)\n",
    "    number = int(match.group(2))\n",
    "    \n",
    "    return letter, number\n",
    "\n",
    "sorted_subgrades = sorted(subgrades, key=custom_sort_key)\n",
    "\n",
    "print(sorted_subgrades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level encoding for sorted sub-grade \n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(sorted_subgrades)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['sub_grade'] = encoder.transform(df['sub_grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "df['sub_grade'].unique()"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
     
    "df = df.drop(remove_col, axis=1)"

   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [

    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all attributes with missing values\n",
    "\n",
    "null_columns = df.columns[df.isnull().any()] \n",
    "null_columns_result = df.isnull().any()[null_columns] \n",
    "null_columns_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emp_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_length'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_lengths = ['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years', 'nan']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder with unique values\n",
    "label_encoder.fit(employment_lengths)\n",
    "\n",
    "# Encode the attribute values\n",
    "df['emp_length'] = label_encoder.transform(df['emp_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emp_length'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mths_since_last_delinq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mths_since_last_delinq'] = df['mths_since_last_delinq'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mths_since_last_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mths_since_last_record'] = df['mths_since_last_record'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### revol_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "revol_util = df['revol_util'].values.reshape(-1,1)\n",
    "\n",
    "revol_util_imputed = imputer.fit_transform(revol_util)\n",
    "\n",
    "df['revol_util'] = revol_util_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### annual_inc_joint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate annual_inc and annual_inc_joint\n",
    "df.loc[df['application_type'] == 'JOINT', 'annual_inc'] = df.loc[df['application_type'] == 'JOINT', 'annual_inc_joint']\n",
    "df = df.drop('annual_inc_joint', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dti_joint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['application_type'] == 'JOINT', 'dti'] = df.loc[df['application_type'] == 'JOINT', 'dti_joint']\n",
    "df = df.drop('dti_joint', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### verification_status_joint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['application_type'] == 'JOINT', 'verification_status'] = df.loc[df['application_type'] == 'JOINT', 'verification_status_joint']\n",
    "df = df.drop('verification_status_joint', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['term'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = [' 36 months', ' 60 months']  # Unique values for encoding\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(term)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['term'] = encoder.transform(df['term'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = ['A', 'B', 'C', 'D', 'E', 'F', 'G']  # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(grades)\n",
    "\n",
    "df['grade'] = encoder.transform(df['grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verification_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veri = ['Not Verified', 'Source Verified', 'Verified']  # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(veri)\n",
    "\n",
    "\n",
    "df['verification_status'] = encoder.transform(df['verification_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pymnt_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan = ['n', 'y']  # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(plan)\n",
    "\n",
    "df['pymnt_plan'] = encoder.transform(df['pymnt_plan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### application_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = ['INDIVIDUAL', 'JOINT'] # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(type)\n",
    "\n",
    "df['application_type'] = encoder.transform(df['application_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initial_list_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "status = ['f', 'w'] # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(status)\n",
    "\n",
    "df['initial_list_status'] = encoder.transform(df['initial_list_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of the following\n",
    "\n",
    "    'open_il_12m', 842681 missing values, can consider scrapping as there's no value\n",
    "    'open_il_24m', 842681 missing values, can consider scrapping as there's no value\n",
    "    'mths_since_rcnt_il', 843035 missing values, while there's alot of missing values however, those recorded >1 months since most recent instalment account opened will 100% not default\n",
    "    'total_bal_il', 842681 missing values, while there's alot of missing values however, those recorded >0 total current balance of all instalment account will 100% not default\n",
    "    'il_util', 844360 missing values, \n",
    "    'open_rv_12m' , 842681 missing values, \n",
    "    'open_rv_24m', 842681 missing values, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 'open_il_24m' & 'open_il_24m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null\n",
    "null_counts = df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Inputation\n",
    "columns_to_visualize = ['open_il_12m', 'open_il_24m']\n",
    "\n",
    "# Plotting settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Frequency of Imputed Values\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create count plots for each column\n",
    "for column in columns_to_visualize:\n",
    "    sns.countplot(data=df, x=column)\n",
    "    \n",
    "plt.legend(columns_to_visualize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['open_il_12m'].unique()\n",
    "df[['open_il_12m', 'open_il_24m']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpute with 0\n",
    "\n",
    "columns_to_impute = ['open_il_12m', 'open_il_24m']\n",
    "\n",
    "# Impute missing values with 0\n",
    "df[columns_to_impute] = df[columns_to_impute].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Inputation\n",
    "columns_to_visualize = ['open_il_12m', 'open_il_24m']\n",
    "\n",
    "# Plotting settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Frequency of Imputed Values\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create count plots for each column\n",
    "for column in columns_to_visualize:\n",
    "    sns.countplot(data=df, x=column)\n",
    "    \n",
    "plt.legend(columns_to_visualize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'mths_since_rcnt_il'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mths_since_rcnt_il'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Inputation\n",
    "columns_to_visualize = ['mths_since_rcnt_il']\n",
    "\n",
    "# Plotting settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Frequency of Imputed Values\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create count plots for each column\n",
    "for column in columns_to_visualize:\n",
    "    sns.countplot(data=df, x=column)\n",
    "    \n",
    "plt.legend(columns_to_visualize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check against default_ind\n",
    "condition = (df['mths_since_rcnt_il'] > 0) & (df['default_ind'] == 1)\n",
    "result_df = df[condition]\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpute with 0\n",
    "\n",
    "columns_to_impute = ['mths_since_rcnt_il']\n",
    "\n",
    "\n",
    "# Impute missing values with 0\n",
    "df[columns_to_impute] = df[columns_to_impute].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Inputation\n",
    "columns_to_visualize = ['mths_since_rcnt_il']\n",
    "\n",
    "# Plotting settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Frequency of Imputed Values\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create count plots for each column\n",
    "for column in columns_to_visualize:\n",
    "    sns.countplot(data=df, x=column)\n",
    "    \n",
    "plt.legend(columns_to_visualize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total_bal_il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.dropna(subset=['total_bal_il'])\n",
    "total_bal_il_values = filtered_df['total_bal_il'].unique()\n",
    "\n",
    "# Display unique values and the rest of the columns\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check against default_ind\n",
    "condition = (df['total_bal_il'] > 0) & (df['default_ind'] == 1)\n",
    "result_df = df[condition]\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Inputation\n",
    "columns_to_visualize = ['total_bal_il']\n",
    "\n",
    "# Plotting settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Frequency of Imputed Values\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create count plots for each column\n",
    "for column in columns_to_visualize:\n",
    "    sns.countplot(data=df, x=column)\n",
    "    \n",
    "plt.legend(columns_to_visualize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpute with 0\n",
    "\n",
    "columns_to_impute = ['total_bal_il']\n",
    "\n",
    "\n",
    "# Impute missing values with 0\n",
    "df[columns_to_impute] = df[columns_to_impute].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Inputation\n",
    "columns_to_visualize = ['total_bal_il']\n",
    "\n",
    "# Plotting settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Frequency of Imputed Values\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create count plots for each column\n",
    "for column in columns_to_visualize:\n",
    "    sns.countplot(data=df, x=column)\n",
    "    \n",
    "plt.legend(columns_to_visualize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.dropna(subset=['il_util'])\n",
    "total_bal_il_values = filtered_df['il_util'].unique()\n",
    "\n",
    "# Display unique values and the rest of the columns\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['il_util'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check against default_ind\n",
    "condition = (df['il_util'] > 0) & (df['default_ind'] == 1)\n",
    "result_df = df[condition]\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before Inputation\n",
    "columns_to_visualize = ['il_util']\n",
    "\n",
    "# Plotting settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Frequency of Imputed Values\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create count plots for each column\n",
    "for column in columns_to_visualize:\n",
    "    sns.countplot(data=df, x=column)\n",
    "    \n",
    "plt.legend(columns_to_visualize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpute with 0\n",
    "\n",
    "columns_to_impute = ['il_util']\n",
    "\n",
    "\n",
    "# Impute missing values with 0\n",
    "df[columns_to_impute] = df[columns_to_impute].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Inputation\n",
    "columns_to_visualize = ['il_util']\n",
    "\n",
    "# Plotting settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Frequency of Imputed Values\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create count plots for each column\n",
    "for column in columns_to_visualize:\n",
    "    sns.countplot(data=df, x=column)\n",
    "    \n",
    "plt.legend(columns_to_visualize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open_rv_12m, open_rv_24m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['open_rv_12m', 'open_rv_24m']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpute with 0\n",
    "\n",
    "columns_to_impute = ['open_rv_12m', 'open_rv_24m']\n",
    "\n",
    "# Impute missing values with 0\n",
    "df[columns_to_impute] = df[columns_to_impute].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_visualize = ['open_rv_12m', 'open_rv_24m']\n",
    "\n",
    "# Plotting settings\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Frequency of Imputed Values\")\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Create count plots for each column\n",
    "for column in columns_to_visualize:\n",
    "    sns.countplot(data=df, x=column)\n",
    "    \n",
    "plt.legend(columns_to_visualize)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the new feature\n",
    "df['open_acc_rate'] = df['open_acc'] / df['total_acc']\n",
    "# Swap the values and column names\n",
    "df['open_acc_rate'], df['default_ind'] = df['default_ind'].copy(), df['open_acc_rate'].copy()\n",
    "df.rename(columns={'open_acc_rate': 'default_ind', 'default_ind': 'open_acc_rate'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Select the correlation values with 'default_ind'\n",
    "target_corr = corr_matrix['default_ind']\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the absolute values of the correlation\n",
    "abs_corr = target_corr.abs()\n",
    "\n",
    "# Sort the absolute correlation values\n",
    "sorted_corr = abs_corr.sort_values(ascending=False)\n",
    "\n",
    "# Display the attribute ranking\n",
    "attribute_ranking = sorted_corr.reset_index()\n",
    "attribute_ranking.columns = ['Attribute', 'Absolute Correlation']\n",
    "attribute_ranking"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
