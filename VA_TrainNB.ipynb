{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data \n",
    "\n",
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the null value in each attributes\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_rev_hi_lim \n",
    "\n",
    "# Fill missing values in total_rev_hi_lim with Simple Imputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "total_rev_hi_lim = df['total_rev_hi_lim'].values.reshape(-1,1)\n",
    "total_rev_hi_lim_imputed = imputer.fit_transform(total_rev_hi_lim)\n",
    "df['total_rev_hi_lim'] = total_rev_hi_lim_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home_ownership \n",
    "\n",
    "# Remove rows with value ANY\n",
    "\n",
    "df = df[df['home_ownership'] != 'ANY']\n",
    "df['home_ownership'].unique()\n",
    "\n",
    "# Level encoding for home ownership \n",
    "\n",
    "home_type = ['RENT', 'OWN', 'MORTGAGE', 'OTHER', 'NONE']  # Unique values for encoding\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(home_type)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['home_ownership'] = encoder.transform(df['home_ownership'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose \n",
    "\n",
    "# Label encoding for purpose\n",
    "\n",
    "df['purpose'].unique()\n",
    "\n",
    "purposes = ['credit_card', 'car', 'small_business', 'other', 'wedding',\n",
    "       'debt_consolidation', 'home_improvement', 'major_purchase',\n",
    "       'medical', 'moving', 'vacation', 'house', 'renewable_energy',\n",
    "       'educational']\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(purposes)\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['purpose'] = encoder.transform(df['purpose'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_grade\n",
    "\n",
    "# Sort the order of subgrades and do label encoding\n",
    "\n",
    "subgrades = ['B2', 'C4', 'C5', 'C1', 'B5', 'A4', 'E1', 'F2', 'C3', 'B1', 'D1',\n",
    "       'A1', 'B3', 'B4', 'C2', 'D2', 'A3', 'A5', 'D5', 'A2', 'E4', 'D3',\n",
    "       'D4', 'F3', 'E3', 'F4', 'F1', 'E5', 'G4', 'E2', 'G3', 'G2', 'G1',\n",
    "       'F5', 'G5']\n",
    "\n",
    "def custom_sort_key(subgrade):\n",
    "    match = re.match(r'([A-Za-z]+)(\\d+)', subgrade)\n",
    "    letter = match.group(1)\n",
    "    number = int(match.group(2))\n",
    "    \n",
    "    return letter, number\n",
    "\n",
    "sorted_subgrades = sorted(subgrades, key=custom_sort_key)\n",
    "\n",
    "# Level encoding for sorted sub-grade \n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(sorted_subgrades)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['sub_grade'] = encoder.transform(df['sub_grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employment_lengths\n",
    "\n",
    "employment_lengths = ['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', '6 years', '7 years', '8 years', '9 years', '10+ years', 'nan']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder with unique values\n",
    "label_encoder.fit(employment_lengths)\n",
    "\n",
    "# Encode the attribute values\n",
    "df['emp_length'] = label_encoder.transform(df['emp_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mths_since_last_delinq\n",
    "\n",
    "df['mths_since_last_delinq'] = df['mths_since_last_delinq'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mths_since_last_record\n",
    "\n",
    "df['mths_since_last_record'] = df['mths_since_last_record'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revol_util\n",
    "# Handle missing value with imputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "revol_util = df['revol_util'].values.reshape(-1,1)\n",
    "\n",
    "revol_util_imputed = imputer.fit_transform(revol_util)\n",
    "\n",
    "df['revol_util'] = revol_util_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate annual_inc and annual_inc_joint\n",
    "\n",
    "df.loc[df['application_type'] == 'JOINT', 'annual_inc'] = df.loc[df['application_type'] == 'JOINT', 'annual_inc_joint']\n",
    "df = df.drop('annual_inc_joint', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dti_joint\n",
    "\n",
    "df.loc[df['application_type'] == 'JOINT', 'dti'] = df.loc[df['application_type'] == 'JOINT', 'dti_joint']\n",
    "df = df.drop('dti_joint', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verification_status_joint\n",
    "\n",
    "df.loc[df['application_type'] == 'JOINT', 'verification_status'] = df.loc[df['application_type'] == 'JOINT', 'verification_status_joint']\n",
    "df = df.drop('verification_status_joint', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term \n",
    "# Label encoding for term\n",
    "\n",
    "term = [' 36 months', ' 60 months']  # Unique values for encoding\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(term)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['term'] = encoder.transform(df['term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verification_status\n",
    "\n",
    "veri = ['Not Verified', 'Source Verified', 'Verified']  # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(veri)\n",
    "\n",
    "\n",
    "df['verification_status'] = encoder.transform(df['verification_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pymnt_plan\n",
    "\n",
    "plan = ['n', 'y']  # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(plan)\n",
    "df['pymnt_plan'] = encoder.transform(df['pymnt_plan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application_type\n",
    "\n",
    "type = ['INDIVIDUAL', 'JOINT'] # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(type)\n",
    "\n",
    "df['application_type'] = encoder.transform(df['application_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initial_list_status\n",
    "\n",
    "status = ['f', 'w'] # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(status)\n",
    "\n",
    "df['initial_list_status'] = encoder.transform(df['initial_list_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop not usable attributes \n",
    "\n",
    "remove_col = [\n",
    "    'id',\n",
    "    'member_id',\n",
    "    'emp_title',\n",
    "    'issue_d',\n",
    "    'desc',\n",
    "    'title',\n",
    "    'zip_code',\n",
    "    'addr_state',\n",
    "    'earliest_cr_line',\n",
    "    'last_pymnt_d',\n",
    "    'last_pymnt_amnt',\n",
    "    'next_pymnt_d',\n",
    "    'last_credit_pull_d',\n",
    "    'collections_12_mths_ex_med',\n",
    "    'mths_since_last_major_derog',\n",
    "    'policy_code',\n",
    "    'tot_coll_amt',\n",
    "    'tot_cur_bal', \n",
    "    'open_acc_6m',\n",
    "    'open_il_6m', \n",
    "    'open_il_12m', \n",
    "    'open_il_24m', \n",
    "    'mths_since_rcnt_il', \n",
    "    'total_bal_il', \n",
    "    'il_util', \n",
    "    'open_rv_12m' ,\n",
    "    'open_rv_24m', \n",
    "    'max_bal_bc', \n",
    "    'all_util', \n",
    "    'inq_fi', \n",
    "    'total_cu_tl', \n",
    "    'inq_last_12m',\n",
    "    'grade'\n",
    "]\n",
    "\n",
    "df = df.drop(remove_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize default_ind attribute\n",
    "\n",
    "sns.countplot(data=df, x='default_ind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New feature using user defined transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CreditUtilizationRatioTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, include_rev_to_inc_ratio=True, include_loan_to_inc_ratio=True, include_repayment_progress=True):\n",
    "        self.revol_bal_col = 'revol_bal'\n",
    "        self.annual_inc_col = 'annual_inc'\n",
    "        self.installment_col = 'installment'\n",
    "        self.total_rec_prncp_col = 'total_rec_prncp'\n",
    "        self.funded_amnt_col = 'funded_amnt'\n",
    "\n",
    "        self.include_rev_to_inc_ratio = include_rev_to_inc_ratio\n",
    "        self.include_loan_to_inc_ratio = include_loan_to_inc_ratio\n",
    "        self.include_repayment_progress = include_repayment_progress\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Find index of columns\n",
    "        revol_bal_ix = np.where(X.columns == self.revol_bal_col)[0][0]\n",
    "        annual_inc_ix = np.where(X.columns == self.annual_inc_col)[0][0]\n",
    "        installment_ix = np.where(X.columns == self.installment_col)[0][0]\n",
    "        total_rec_prncp_ix = np.where(X.columns == self.total_rec_prncp_col)[0][0]\n",
    "        funded_amnt_ix = np.where(X.columns == self.funded_amnt_col)[0][0]\n",
    "\n",
    "        # Calculate the Revolving Credit Balance to Annual Income Ratio.\n",
    "        rev_to_inc_ratio = X.iloc[:, revol_bal_ix] / X.iloc[:, annual_inc_ix]\n",
    "\n",
    "        # Calculate the Loan Payment-to-Income Ratio.\n",
    "        loan_to_inc_ratio = X.iloc[:, installment_ix] / (X.iloc[:, annual_inc_ix] / 12)\n",
    "\n",
    "        # Calculate the Repayment Progress.\n",
    "        repayment_progress = (X.iloc[:, total_rec_prncp_ix] / X.iloc[:, funded_amnt_ix]) * 100\n",
    "\n",
    "        if self.include_rev_to_inc_ratio:\n",
    "            # Add the calculated Revolving Credit Balance to Annual Income Ratio as a new column to the input data.\n",
    "            X['Rev_to_Inc_Ratio'] = rev_to_inc_ratio\n",
    "\n",
    "        if self.include_loan_to_inc_ratio:\n",
    "            # Add the calculated Loan Payment-to-Income Ratio as a new column to the input data.\n",
    "            X['Loan_Payment_to_Income_Ratio'] = loan_to_inc_ratio\n",
    "\n",
    "        if self.include_repayment_progress:\n",
    "            # Add the calculated Repayment Progress as a new column to the input data.\n",
    "            X['Repayment_Progress'] = repayment_progress\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - Correlation matrix\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Select the correlation values with 'default_ind'\n",
    "target_corr = corr_matrix['default_ind']\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a correlation matrix named 'corr_matrix' and the target variable is 'default_ind'\n",
    "target_corr = corr_matrix['default_ind']\n",
    "\n",
    "# Find absolute correlation values and sort in ascending order\n",
    "target_corr_abs_sorted = target_corr.abs().sort_values()\n",
    "\n",
    "print(target_corr_abs_sorted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (subset of columns from the original dataset)\n",
    "selected_features = ['recoveries', 'collection_recovery_fee','out_prncp','out_prncp_inv', 'Repayment_Progress', 'sub_grade']\n",
    "\n",
    "# Choose features based on the corr values \n",
    "# selected_features = ['recoveries', 'collection_recovery_fee', 'out_prncp', 'out_prncp_inv', 'int_rate', 'total_rec_late_fee', 'sub_grade']\n",
    "\n",
    "# Create the feature matrix X by selecting the columns from the original dataset\n",
    "# y for default_ind\n",
    "X = df.iloc[:, :-1]\n",
    "y = df['default_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StratifiedShuffleSplit Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Instantiate the StratifiedShuffleSplit object\n",
    "stratified_shuffle_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform the split and obtain the indices for training and testing sets\n",
    "for train_index, test_index in stratified_shuffle_split.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Instantiate the StratifiedShuffleSplit object\n",
    "stratified_shuffle_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform the split and obtain the indices for training and testing sets\n",
    "for train_index, test_index in stratified_shuffle_split.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# Combine the training features and target into a single DataFrame\n",
    "training_data = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Separate the minority and majority classes in the training data\n",
    "minority_class = training_data[training_data['default_ind'] == '1']\n",
    "majority_class = training_data[training_data['default_ind'] == '0']\n",
    "\n",
    "# Undersample the majority class to balance the class distribution within the training data\n",
    "undersampled_majority_class = majority_class.sample(len(minority_class), random_state=42)\n",
    "\n",
    "# Combine the undersampled majority class and the minority class to form the final training set\n",
    "undersampled_training_data = pd.concat([undersampled_majority_class, minority_class])\n",
    "\n",
    "# Shuffle the undersampled training data\n",
    "undersampled_training_data = undersampled_training_data.sample(frac=1, random_state=42)\n",
    "\n",
    "# Separate the features and target after undersampling\n",
    "X_train_undersampled = undersampled_training_data.drop('default_ind', axis=1)\n",
    "y_train_undersampled = undersampled_training_data['default_ind']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Create a Gaussian Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb_classifier.fit(X_train_undersampled, y_train_undersampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide to use new generated features\n",
    "\n",
    "transformer_with_features = CreditUtilizationRatioTransformer(\n",
    "                                                            include_rev_to_inc_ratio=True,\n",
    "                                                            include_loan_to_inc_ratio=True,\n",
    "                                                            include_repayment_progress=True)\n",
    "\n",
    "# Apply the transformer to add the new features to df.\n",
    "X = transformer_with_features.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-default', 'Default'], yticklabels=['Non-default', 'Default'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Gaussian Naive Bayes Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
