{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import datetime\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data \n",
    "\n",
    "df = pd.read_csv('../data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for later use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified sampling\n",
    "\n",
    "def StratifiedSampling(X,y, test_size=0.2, random_state=42):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, \n",
    "                                                        random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Oversampling(X, y, test_size=0.2, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, stratify=y, \n",
    "                                                        random_state=random_state)\n",
    "    # Instantiate the SMOTE object\n",
    "    smote = SMOTE(random_state=42)\n",
    "    # Perform SMOTE only on the training data\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate_predictions function\n",
    "#### for metric presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred, model_name):\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Display the confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-default', 'Default'], \n",
    "                yticklabels=['Non-default', 'Default'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=['Non-default', 'Default'], output_dict=True)\n",
    "\n",
    "    # Create a summary table\n",
    "    summary_table = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'ROC-AUC Score': [roc_auc],\n",
    "        'Precision (Non-default)': [report['Non-default']['precision']],\n",
    "        'Recall (Non-default)': [report['Non-default']['recall']],\n",
    "        'F1-score (Non-default)': [report['Non-default']['f1-score']],\n",
    "        'Precision (Default)': [report['Default']['precision']],\n",
    "        'Recall (Default)': [report['Default']['recall']],\n",
    "        'F1-score (Default)': [report['Default']['f1-score']],\n",
    "    })\n",
    "\n",
    "    return summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the null value in each attributes\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_rev_hi_lim \n",
    "\n",
    "# Fill missing values in total_rev_hi_lim with Simple Imputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "total_rev_hi_lim = df['total_rev_hi_lim'].values.reshape(-1,1)\n",
    "total_rev_hi_lim_imputed = imputer.fit_transform(total_rev_hi_lim)\n",
    "df['total_rev_hi_lim'] = total_rev_hi_lim_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home_ownership \n",
    "\n",
    "# Remove rows with value ANY\n",
    "\n",
    "df = df[df['home_ownership'] != 'ANY']\n",
    "df['home_ownership'].unique()\n",
    "\n",
    "# Level encoding for home ownership \n",
    "\n",
    "home_type = ['RENT', 'OWN', 'MORTGAGE', 'OTHER', 'NONE']  # Unique values for encoding\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(home_type)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['home_ownership'] = encoder.transform(df['home_ownership'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose \n",
    "\n",
    "# Label encoding for purpose\n",
    "\n",
    "df['purpose'].unique()\n",
    "\n",
    "purposes = ['credit_card', 'car', 'small_business', 'other', 'wedding',\n",
    "       'debt_consolidation', 'home_improvement', 'major_purchase',\n",
    "       'medical', 'moving', 'vacation', 'house', 'renewable_energy',\n",
    "       'educational']\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(purposes)\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['purpose'] = encoder.transform(df['purpose'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_grade\n",
    "\n",
    "# Sort the order of subgrades and do label encoding\n",
    "\n",
    "subgrades = ['B2', 'C4', 'C5', 'C1', 'B5', 'A4', 'E1', 'F2', 'C3', 'B1', 'D1',\n",
    "       'A1', 'B3', 'B4', 'C2', 'D2', 'A3', 'A5', 'D5', 'A2', 'E4', 'D3',\n",
    "       'D4', 'F3', 'E3', 'F4', 'F1', 'E5', 'G4', 'E2', 'G3', 'G2', 'G1',\n",
    "       'F5', 'G5']\n",
    "\n",
    "def custom_sort_key(subgrade):\n",
    "    match = re.match(r'([A-Za-z]+)(\\d+)', subgrade)\n",
    "    letter = match.group(1)\n",
    "    number = int(match.group(2))\n",
    "    \n",
    "    return letter, number\n",
    "\n",
    "sorted_subgrades = sorted(subgrades, key=custom_sort_key)\n",
    "\n",
    "# Level encoding for sorted sub-grade \n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(sorted_subgrades)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['sub_grade'] = encoder.transform(df['sub_grade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employment_lengths\n",
    "\n",
    "employment_lengths = ['< 1 year', '1 year', '2 years', '3 years', '4 years', '5 years', '6 years', \n",
    "                        '7 years', '8 years', '9 years', '10+ years', 'nan']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder with unique values\n",
    "label_encoder.fit(employment_lengths)\n",
    "\n",
    "# Encode the attribute values\n",
    "df['emp_length'] = label_encoder.transform(df['emp_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mths_since_last_delinq\n",
    "\n",
    "df['mths_since_last_delinq'] = df['mths_since_last_delinq'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mths_since_last_record\n",
    "\n",
    "df['mths_since_last_record'] = df['mths_since_last_record'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revol_util\n",
    "# Handle missing value with imputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "revol_util = df['revol_util'].values.reshape(-1,1)\n",
    "\n",
    "revol_util_imputed = imputer.fit_transform(revol_util)\n",
    "\n",
    "df['revol_util'] = revol_util_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate annual_inc and annual_inc_joint\n",
    "\n",
    "df.loc[df['application_type'] \n",
    "== 'JOINT', 'annual_inc'] = df.loc[df['application_type'] == 'JOINT', 'annual_inc_joint']\n",
    "\n",
    "df = df.drop('annual_inc_joint', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dti_joint\n",
    "\n",
    "df.loc[df['application_type'] \n",
    "       == 'JOINT', 'dti'] = df.loc[df['application_type'] == 'JOINT', 'dti_joint']\n",
    "df = df.drop('dti_joint', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verification_status_joint\n",
    "\n",
    "df.loc[df['application_type'] \n",
    "== 'JOINT', 'verification_status'] = df.loc[df['application_type'] == 'JOINT', 'verification_status_joint']\n",
    "df = df.drop('verification_status_joint', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term \n",
    "# Label encoding for term\n",
    "\n",
    "term = [' 36 months', ' 60 months']  # Unique values for encoding\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the grades\n",
    "encoder.fit(term)\n",
    "\n",
    "# Encode the 'grade' column in the DataFrame\n",
    "df['term'] = encoder.transform(df['term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verification_status\n",
    "\n",
    "veri = ['Not Verified', 'Source Verified', 'Verified']  # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(veri)\n",
    "\n",
    "\n",
    "df['verification_status'] = encoder.transform(df['verification_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pymnt_plan\n",
    "\n",
    "plan = ['n', 'y']  # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(plan)\n",
    "df['pymnt_plan'] = encoder.transform(df['pymnt_plan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# application_type\n",
    "\n",
    "type = ['INDIVIDUAL', 'JOINT'] # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(type)\n",
    "\n",
    "df['application_type'] = encoder.transform(df['application_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initial_list_status\n",
    "\n",
    "status = ['f', 'w'] # Unique values for encoding\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "encoder.fit(status)\n",
    "\n",
    "df['initial_list_status'] = encoder.transform(df['initial_list_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing last credit pull\n",
    "df['last_credit_pull_d'].fillna(\"01-01-2016\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit History Length:\n",
    "# Calculated as: last_credit_pull - earliest_cr_line\n",
    "def date_difference(date_str1, date_str2):\n",
    "    # Convert date strings to datetime objects\n",
    "    date_format = \"%d-%m-%Y\"\n",
    "    date1 = datetime.strptime(date_str1, date_format)\n",
    "    date2 = datetime.strptime(date_str2, date_format)\n",
    "\n",
    "    # Calculate the difference\n",
    "    difference = date2 - date1\n",
    "\n",
    "    # Return the difference in days\n",
    "    return difference.days\n",
    "\n",
    "df['credit_history_length'] = df.apply(lambda row: date_difference(row['earliest_cr_line'],\n",
    "                                         row['last_credit_pull_d']), axis=1)\n",
    "\n",
    "# Swap the values and column names\n",
    "df['default_ind'], df['credit_history_length'] = df['credit_history_length'], df['default_ind']\n",
    "df.rename(columns={'default_ind': 'credit_history_length', 'credit_history_length': 'default_ind'}, \n",
    "                    inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop not usable attributes \n",
    "\n",
    "remove_col = [\n",
    "    'id',\n",
    "    'member_id',\n",
    "    'emp_title',\n",
    "    'issue_d',\n",
    "    'desc',\n",
    "    'title',\n",
    "    'zip_code',\n",
    "    'addr_state',\n",
    "    'earliest_cr_line',\n",
    "    'last_pymnt_d',\n",
    "    'last_pymnt_amnt',\n",
    "    'next_pymnt_d',\n",
    "    'last_credit_pull_d',\n",
    "    'collections_12_mths_ex_med',\n",
    "    'mths_since_last_major_derog',\n",
    "    'policy_code',\n",
    "    'tot_coll_amt',\n",
    "    'tot_cur_bal', \n",
    "    'open_acc_6m',\n",
    "    'open_il_6m', \n",
    "    'open_il_12m', \n",
    "    'open_il_24m', \n",
    "    'mths_since_rcnt_il', \n",
    "    'total_bal_il', \n",
    "    'il_util', \n",
    "    'open_rv_12m' ,\n",
    "    'open_rv_24m', \n",
    "    'max_bal_bc', \n",
    "    'all_util', \n",
    "    'inq_fi', \n",
    "    'total_cu_tl', \n",
    "    'inq_last_12m',\n",
    "    'grade'\n",
    "]\n",
    "\n",
    "df = df.drop(remove_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize default_ind attribute\n",
    "\n",
    "sns.countplot(data=df, x='default_ind')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New feature using user defined transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditUtilizationRatioTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, include=True):\n",
    "        self.revol_bal_col = 'revol_bal'\n",
    "        self.annual_inc_col = 'annual_inc'\n",
    "        self.installment_col = 'installment'\n",
    "        self.total_rec_prncp_col = 'total_rec_prncp'\n",
    "        self.funded_amnt_col = 'funded_amnt'\n",
    "\n",
    "        self.include_rev_to_inc_ratio = include\n",
    "        self.include_loan_to_inc_ratio = include\n",
    "        self.include_repayment_progress = include\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Find index of columns\n",
    "        revol_bal_ix = np.where(X.columns == self.revol_bal_col)[0][0]\n",
    "        annual_inc_ix = np.where(X.columns == self.annual_inc_col)[0][0]\n",
    "        installment_ix = np.where(X.columns == self.installment_col)[0][0]\n",
    "        total_rec_prncp_ix = np.where(X.columns == self.total_rec_prncp_col)[0][0]\n",
    "        funded_amnt_ix = np.where(X.columns == self.funded_amnt_col)[0][0]\n",
    "\n",
    "        # Calculate the Revolving Credit Balance to Annual Income Ratio.\n",
    "        rev_to_inc_ratio = X.iloc[:, revol_bal_ix] / X.iloc[:, annual_inc_ix]\n",
    "\n",
    "        # Calculate the Loan Payment-to-Income Ratio.\n",
    "        loan_to_inc_ratio = X.iloc[:, installment_ix] / (X.iloc[:, annual_inc_ix] / 12)\n",
    "\n",
    "        # Calculate the Repayment Progress.\n",
    "        repayment_progress = (X.iloc[:, total_rec_prncp_ix] / X.iloc[:, funded_amnt_ix]) * 100\n",
    "\n",
    "        # Create a copy of the original DataFrame to avoid modifying it directly\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Add the new columns to the DataFrame using .loc\n",
    "        if self.include_rev_to_inc_ratio:\n",
    "            X_transformed.loc[:, 'Rev_to_Inc_Ratio'] = rev_to_inc_ratio\n",
    "\n",
    "        if self.include_loan_to_inc_ratio:\n",
    "            X_transformed.loc[:, 'Loan_Payment_to_Income_Ratio'] = loan_to_inc_ratio\n",
    "\n",
    "        if self.include_repayment_progress:\n",
    "            X_transformed.loc[:, 'Repayment_Progress'] = repayment_progress\n",
    "\n",
    "        return X_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - Correlation matrix\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# Select the correlation values with 'default_ind'\n",
    "target_corr = corr_matrix['default_ind']\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features (subset of columns from the original dataset)\n",
    "selected_features = [\n",
    "    'collection_recovery_fee'   ,                  \n",
    "    'acc_now_delinq' ,                                    \n",
    "    'funded_amnt'   ,               \n",
    "    'funded_amnt_inv'   ,            \n",
    "    'mths_since_last_record' ,       \n",
    "    'delinq_2yrs'   ,                          \n",
    "    'dti'            ,              \n",
    "    'mths_since_last_delinq'  ,       \n",
    "    'emp_length',                      \n",
    "    'pub_rec'   ,                  \n",
    "    'revol_bal'    ,                                \n",
    "    'credit_history_length'  ,        \n",
    "    'term'   ,                       \n",
    "    'home_ownership' ,                                \n",
    "    'total_rev_hi_lim'  ,              \n",
    "    'total_pymnt'      ,                     \n",
    "    'total_pymnt_inv'  ,              \n",
    "    'purpose'   ,                   \n",
    "    'revol_util'   ,                 \n",
    "    'total_rec_int'   ,               \n",
    "    'inq_last_6mths'  ,              \n",
    "    'total_rec_prncp' ,                        \n",
    "    'sub_grade'        ,             \n",
    "    'total_rec_late_fee'   ,          \n",
    "    'int_rate'   ,                    \n",
    "    'out_prncp_inv'  ,                 \n",
    "    'out_prncp'   ,                       \n",
    "    'recoveries'                     \n",
    "]\n",
    "\n",
    "\n",
    "# Define the features (subset of columns from the original dataset)\n",
    "# selected_features = ['recoveries', 'collection_recovery_fee', 'out_prncp', 'out_prncp_inv', 'sub_grade', 'revol_bal', 'annual_inc', 'installment', 'total_rec_prncp', 'funded_amnt']\n",
    "\n",
    "# Create the feature matrix X by selecting the columns from the original dataset\n",
    "X_before_transformed = df[selected_features]\n",
    "y = df['default_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StratifiedShuffleSplit Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_X_train, before_X_test, before_y_train, before_y_test = StratifiedSampling(X_before_transformed, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation of Test and Train shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"before_X_train shape:\", before_X_train.shape)\n",
    "print(\"before_X_test shape:\", before_X_test.shape)\n",
    "print(\"before_y_train shape:\", before_y_train.shape)\n",
    "print(\"before_y_test shape:\", before_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result: Significantly imbalanced class distribution\n",
    "\n",
    "#### Solution: Have to apply class balancing techniques (e.g. oversampling/SMOTE, or Class-Weighted approach) just before Model training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add User-defined Transformer Features to X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide to use new generated features\n",
    "\n",
    "transformer_with_features = CreditUtilizationRatioTransformer(include=True)\n",
    "\n",
    "# Apply the transformer to add the new features to df.\n",
    "new_df = transformer_with_features.transform(df)\n",
    "new_selected_features = selected_features\n",
    "\n",
    "new_selected_features.append('Repayment_Progress')\n",
    "new_selected_features.append('Loan_Payment_to_Income_Ratio')\n",
    "new_selected_features.append('Rev_to_Inc_Ratio')\n",
    "X_transformed = new_df[new_selected_features]\n",
    "# new_y1 = NB_df['default_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### StratifiedShuffleSplit Re-Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Balanced_X_train, Balanced_X_test, Balanced_y_train, Balanced_y_test = StratifiedSampling(X_transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the StratifiedShuffleSplit object\n",
    "# stratified_shuffle_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Perform the split and obtain the indices for training and testing sets\n",
    "# for train_index, test_index in stratified_shuffle_split.split(X_transformed, y):\n",
    "#     Balanced_X_train, Balanced_X_test = X_transformed.iloc[train_index], X_transformed.iloc[test_index]\n",
    "#     Balanced_y_train, Balanced_y_test = y.iloc[train_index], y.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisation of Test and Train shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balanced_X_train shape:\", Balanced_X_train.shape)\n",
    "print(\"Balanced_X_test shape:\", Balanced_X_test.shape)\n",
    "print(\"Balanced_y_train shape:\", Balanced_y_train.shape)\n",
    "print(\"Balanced_y_test shape:\", Balanced_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Fresh round of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert into NP array for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_before_X_test = np.array(before_X_test)\n",
    "KNN_Balance_X_test = np.array(Balanced_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic KNN Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_basic_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "KNN_basic_classifier.fit(before_X_train, before_y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "KNN_basic_y_pred = KNN_basic_classifier.predict(KNN_before_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Fine tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a: Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the nearest n_neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Define the range of n_neighbors to test\n",
    "param_grid = {'n_neighbors': range(3, 6)}\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "KNNclassifier = KNeighborsClassifier(weights='distance')\n",
    "\n",
    "# Use GridSearchCV or RandomizedSearchCV for hyperparameter tuning\n",
    "# For GridSearchCV:\n",
    "grid_search = GridSearchCV(KNNclassifier, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(Balanced_X_train, Balanced_y_train)\n",
    "\n",
    "# For RandomizedSearchCV:\n",
    "# randomized_search = RandomizedSearchCV(classifier, param_distributions=param_grid, \n",
    "# cv=5, scoring='accuracy', n_jobs=-1, n_iter=10)\n",
    "\n",
    "# randomized_search.fit(Balanced_X_train, Balanced_y_train)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding performance\n",
    "print(\"Best n_neighbors:\", grid_search.best_params_['n_neighbors'])\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b: Training KNN Model using SMOTE approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Perform SMOTE only on the training data\n",
    "KNN_balanced_X_train, KNN_balanced_y_train= smote.fit_resample(Balanced_X_train, Balanced_y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualising Data Shape Before and After SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before SMOTE\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(pd.Series(Balanced_y_train).value_counts())\n",
    "\n",
    "# After SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(KNN_balanced_y_train).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier on the training data with SMOTE\n",
    "KNN_classifier_smote = KNeighborsClassifier(n_neighbors=4)\n",
    "KNN_classifier_smote.fit(Balanced_X_train, Balanced_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using the trained model\n",
    "KNN_smote_y_pred = KNN_classifier_smote.predict(KNN_Balance_X_test)\n",
    "\n",
    "# Calculate evaluation metrics for the test set\n",
    "accuracy_smote = accuracy_score(Balanced_y_test, KNN_smote_y_pred)\n",
    "roc_auc_smote = roc_auc_score(Balanced_y_test, KNN_smote_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d: Training KNN Model using Class-Weighted approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training  Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set the weights parameter to 'distance'\n",
    "# 'distance' means weights are assigned based on the inverse of the distance\n",
    "class_classifier = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "\n",
    "# Train the classifier on the training data with class weights\n",
    "class_classifier.fit(Balanced_X_train, Balanced_y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing data\n",
    "KNN_class_y_pred = class_classifier.predict(KNN_Balance_X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "class_accuracy = accuracy_score(Balanced_y_test, KNN_class_y_pred)\n",
    "roc_auc = roc_auc_score(Balanced_y_test, KNN_class_y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Evaluate the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls evalutation)predictions() to display metric results\n",
    "basic_summary = evaluate_predictions(Balanced_y_test, KNN_basic_y_pred, \"Basic Model\")\n",
    "smote_summary = evaluate_predictions(Balanced_y_test, KNN_smote_y_pred, \"Balanced Model\")\n",
    "class_summary = evaluate_predictions(Balanced_y_test, KNN_class_y_pred, \"Class Model\")\n",
    "\n",
    "# Combine all summaries into one DataFrame\n",
    "all_summaries = pd.concat([basic_summary, smote_summary, class_summary], ignore_index=True)\n",
    "\n",
    "# Display the summary table\n",
    "print(all_summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3b: \n",
    "#### Considerations: \n",
    "#### For predicting 'default_rate'\n",
    "#### Overall Accuracy - The overall measure on how well the model predicts both the positive and negative rate. Class Model does have an edge here.\n",
    "#### Recall - This is important as the cost of a false-negative is high, especially in our case of loan default prediction, a false negative would mean that the model fails to identify a loan that will actually default. This could result in potential losses for the lender. \n",
    "#### Conclusion: Since Class-Weighted approach have better scores across almost all metrics, therefore it is best to use this approach\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fresh Round of Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a: Basic Decision Tree Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Decision Tree Classifier with random state = 42\n",
    "DT_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier\n",
    "DT_classifier.fit(before_X_train, before_y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "DT_y_pred_basic = DT_classifier.predict(before_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b: Basic decision tree model + SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the SMOTE object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Perform SMOTE only on the training data\n",
    "DT_X_train_balance, DT_X_test, DT_y_train_balance, DT_y_test = Oversampling(X_before_transformed, y)\n",
    "\n",
    "# Create a Decision Tree Classifier with random state = 42\n",
    "DT_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "DT_classifier.fit(DT_X_train_balance, DT_y_train_balance)\n",
    "\n",
    "# Make predictions on the test set\n",
    "DT_y_pred_basic_smote = DT_classifier.predict(DT_X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Fine tune DT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a: Training Decision Tree Model with hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_classifier = DecisionTreeClassifier(criterion='entropy',splitter = 'best', max_depth = 10, \n",
    "                                       min_samples_split = 10, min_samples_leaf = 4, \n",
    "                                       class_weight='balanced', random_state=42)\n",
    "\n",
    "DT_classifier.fit(DT_X_train_balance, DT_y_train_balance)\n",
    "\n",
    "DT_y_pred = DT_classifier.predict(DT_X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model - Pre prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of ccp_alpha to test\n",
    "DT_param_grid = {'max_depth': range(3, 9), 'criterion': ['gini', 'entropy', 'log_loss'] }\n",
    "\n",
    "# Initialize the DT classifier\n",
    "DT_classifier_prepruned = DecisionTreeClassifier(splitter = 'best', min_samples_split = 10, min_samples_leaf = 4, class_weight='balanced', random_state=42)\n",
    "\n",
    "# Use GridSearchCV or RandomizedSearchCV for hyperparameter tuning\n",
    "# For GridSearchCV:\n",
    "grid_search = GridSearchCV(DT_classifier_prepruned, DT_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "best_pruned_model = grid_search.fit(DT_X_train_balance, DT_y_train_balance)\n",
    "\n",
    "DT_y_pred_prepruned = best_pruned_model.predict(DT_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b: Training Decision Tree Model with post-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of ccp_alpha to test\n",
    "DT_param_grid = {'ccp_alpha': range(0, 5)}\n",
    "\n",
    "# Initialize the DT classifier\n",
    "DT_classifier_postpruned = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "\n",
    "# Use GridSearchCV or RandomizedSearchCV for hyperparameter tuning\n",
    "# For GridSearchCV:\n",
    "grid_search = GridSearchCV(DT_classifier_postpruned, DT_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "best_pruned_model = grid_search.fit(DT_X_train_balance, DT_y_train_balance)\n",
    "\n",
    "DT_y_predict = best_pruned_model.predict(DT_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Evaluate the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_basic_summary = evaluate_predictions(DT_y_test, DT_y_pred_basic, \"Basic Model\")\n",
    "DT_basic_SMOTE_summary = evaluate_predictions(DT_y_test, DT_y_pred_basic_smote, \"Basic SMOTE Model\")\n",
    "DT_prepruned_summary = evaluate_predictions(DT_y_test, DT_y_pred_prepruned, \"Pre-Pruning Model\")\n",
    "DT_postpruned_summary = evaluate_predictions(DT_y_test, DT_y_predict, \"Post-Pruning Model\")\n",
    "\n",
    "\n",
    "# Combine all summaries into one DataFrame\n",
    "DT_all_summaries = pd.concat([DT_basic_summary,DT_basic_SMOTE_summary, DT_prepruned_summary, DT_postpruned_summary], ignore_index=True)\n",
    "# Display the summary table\n",
    "print(DT_all_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: Since Decision Tree using SMOTE have the best scores across almost all metrics, therefore it is best to use this approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NB Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Fresh round of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c: Naive Bayes Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_Xraw_train, NB_Xraw_test, NB_yraw_train, NB_yraw_test = StratifiedSampling(X_before_transformed, y)\n",
    "\n",
    "nb_classifier_default = GaussianNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb_classifier_default.fit(NB_Xraw_train, NB_yraw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_yraw_pred = nb_classifier_default.predict(NB_Xraw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified + Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_X_train, NB_X_test, NB_y_train, NB_y_test = Oversampling(X_before_transformed, y)\n",
    "nb_classifier_default = GaussianNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb_classifier_default.fit(NB_X_train, NB_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_y_pred = nb_classifier_default.predict(NB_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Fine tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a: + User-defined Transformer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide to use new generated features\n",
    "\n",
    "NB_transformer_with_features = CreditUtilizationRatioTransformer(include=True)\n",
    "\n",
    "# Apply the transformer to add the new features to df.\n",
    "NB_df = NB_transformer_with_features.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_selected_features = selected_features\n",
    "\n",
    "NB_selected_features.append('Repayment_Progress')\n",
    "NB_selected_features.append('Loan_Payment_to_Income_Ratio')\n",
    "NB_selected_features.append('Rev_to_Inc_Ratio')\n",
    "NB_X1 = NB_df[NB_selected_features]\n",
    "NB_y1 = NB_df['default_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b: Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gaussian Naive Bayes classifier\n",
    "NB_X1_train, NB_X1_test, NB_y1_train, NB_y1_test = Oversampling(NB_X1, NB_y1)\n",
    "\n",
    "NB_param_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "}\n",
    "\n",
    "nb_classifier_hyper = GaussianNB()\n",
    "NB_grid_search = GridSearchCV(estimator=nb_classifier_hyper, param_grid=NB_param_grid, cv=5)\n",
    "NB_grid_search.fit(NB_X1_train, NB_y1_train)\n",
    "\n",
    "# Step 7: Access the results\n",
    "NB_best_params = NB_grid_search.best_params_\n",
    "NB_best_model = NB_grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_y_pred_hyper = NB_best_model.predict(NB_X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Evaluate the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred, model_name):\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Display the confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-default', 'Default'], \n",
    "                yticklabels=['Non-default', 'Default'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=['Non-default', 'Default'], output_dict=True)\n",
    "\n",
    "    # Create a summary table\n",
    "    summary_table = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'ROC-AUC Score': [roc_auc],\n",
    "        'Precision (Non-default)': [report['Non-default']['precision']],\n",
    "        'Recall (Non-default)': [report['Non-default']['recall']],\n",
    "        'F1-score (Non-default)': [report['Non-default']['f1-score']],\n",
    "        'Precision (Default)': [report['Default']['precision']],\n",
    "        'Recall (Default)': [report['Default']['recall']],\n",
    "        'F1-score (Default)': [report['Default']['f1-score']],\n",
    "    })\n",
    "\n",
    "    return summary_table\n",
    "\n",
    "# Usage:\n",
    "Raw_summary = evaluate_predictions(NB_yraw_test, NB_yraw_pred, \"Raw Model\")\n",
    "Default_summary = evaluate_predictions(NB_y_test, NB_y_pred, \"Default Model\")\n",
    "FineTune_summary = evaluate_predictions(NB_y1_test, NB_y_pred_hyper, \"FineTuned Model\")\n",
    "\n",
    "\n",
    "# Combine all summaries into one DataFrame\n",
    "all_summaries = pd.concat([Raw_summary, Default_summary, FineTune_summary], ignore_index=True)\n",
    "\n",
    "# Display the summary table\n",
    "print(all_summaries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b: Conclusion\n",
    "\n",
    "### Data preprocessing\n",
    "\n",
    "For data pre-processing: Besides stratified sampling, we used oversampling since the data is imbalanced, resulting in an unequal distribution of default = 1 and default = 0 values. We observed a significant improvement after implementing oversampling: <br><br>\n",
    "<small> Include the figure for accuracy and chunk of code that is used to calculate accuracy</small>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the confusion matrix, the number of wrong predictions for label 1 decreased from <b>4205</b> to <b>4142</b>. Although the overall accuracy decreased, it is acceptable in this case because our main objective is to identify as many default cases as possible. By doing so, the organization can pinpoint customers who are more likely to have a default value of 1. <br>\n",
    "\n",
    "<small>Include the figure for confusion matrix and can explain the number for true positive and false positive </small>\n",
    "\n",
    "<br>\n",
    "    \n",
    "### Model fine - tuning\n",
    "\n",
    "Accuracy of default NB: 0.9710562286061427 <br>\n",
    "Accuracy of default NB (Fine tune): 0.98836407818031 <br>\n",
    "\n",
    "We can see that after the fine tuning, the accuracry of the model increased. Our intention is to predict as much number of true default as possible. The first thing we do in model fine tuning is to implement the new features that is generated in user defined transformer. The next thing we can tune for NB is var smoothing. This hyperparameter is used to handle numerical stability issues when a feature has zero variance. The default value is 1e-9, and it ensures that no feature has zero variance. By adjusting this parameter, I can control the amount of smoothing applied to the variances of the individual features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Fresh round of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a: RF classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_X_train_balanced, RF_X_test, RF_y_train_balanced, RF_y_test = Oversampling(X_before_transformed, y)\n",
    "\n",
    "# will take 1 min to train\n",
    "rf_classifier = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "rf_classifier.fit(RF_X_train_balanced, RF_y_train_balanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_y_pred = rf_classifier.predict(RF_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b: StratifiedShuffleSplit Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_X2_train, RF_X2_test, RF_y2_train, RF_y2_test = train_test_split(X_before_transformed, y, test_size=0.2, stratify=y, \n",
    "                                                                    random_state=42)\n",
    "                                                                    \n",
    "rf_classifier_default = RandomForestClassifier(random_state=42, n_estimators=9)\n",
    "rf_classifier_default.fit(RF_X2_train, RF_y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_y_pred_default = rf_classifier_default.predict(RF_X2_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Fine tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a: + User-defined Transformer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide to use new generated features\n",
    "\n",
    "RF_transformer_with_features = CreditUtilizationRatioTransformer(include=True)\n",
    "\n",
    "# Apply the transformer to add the new features to df.\n",
    "RF_df = RF_transformer_with_features.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_selected_features = selected_features \n",
    "\n",
    "RF_selected_features.append('Repayment_Progress')\n",
    "RF_selected_features.append('Loan_Payment_to_Income_Ratio')\n",
    "RF_selected_features.append('Rev_to_Inc_Ratio')\n",
    "RF_X1 = RF_df[RF_selected_features]\n",
    "RF_y1 = RF_df['default_ind']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b: Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_X1_train_balanced, RF_X1_test, RF_y1_train_balanced, RF_y1_test = Oversampling(RF_X1, RF_y1)\n",
    "\n",
    "# around 4 mins to train\n",
    "rf_classifier_tuned = RandomForestClassifier(n_estimators=9, random_state=42)\n",
    "RF_param_grid = {\n",
    "    'max_depth': [5, 6, 7, 8, 9],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "RF_grid_search = GridSearchCV(rf_classifier_tuned, RF_param_grid, cv=5, scoring='f1', verbose=1, n_jobs=-1)\n",
    "RF_best_model = RF_grid_search.fit(RF_X1_train_balanced, RF_y1_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_y_pred_tuned = RF_best_model.predict(RF_X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Evaluate the outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a: Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred, model_name):\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Display the confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Non-default', 'Default'], \n",
    "                yticklabels=['Non-default', 'Default'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "    # Generate the classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=['Non-default', 'Default'], output_dict=True)\n",
    "\n",
    "    # Create a summary table\n",
    "    summary_table = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'Accuracy': [accuracy],\n",
    "        'ROC-AUC Score': [roc_auc],\n",
    "        'Precision (Non-default)': [report['Non-default']['precision']],\n",
    "        'Recall (Non-default)': [report['Non-default']['recall']],\n",
    "        'F1-score (Non-default)': [report['Non-default']['f1-score']],\n",
    "        'Precision (Default)': [report['Default']['precision']],\n",
    "        'Recall (Default)': [report['Default']['recall']],\n",
    "        'F1-score (Default)': [report['Default']['f1-score']],\n",
    "    })\n",
    "\n",
    "    return summary_table\n",
    "\n",
    "# Usage:\n",
    "basic_summary = evaluate_predictions(RF_y_test, RF_y_pred_default, \"Basic Model\")\n",
    "balanced_summary = evaluate_predictions(RF_y_test, RF_y_pred, \"Balanced Model\")\n",
    "tuned_summary = evaluate_predictions(RF_y_test, RF_y_pred_tuned, \"Fine-Tuned Model\")\n",
    "\n",
    "# Combine all summaries into one DataFrame\n",
    "all_summaries = pd.concat([basic_summary, balanced_summary, tuned_summary], ignore_index=True)\n",
    "print(all_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the result, we can see that even with fine - tuning, the accuracy of random forest model is high, around 99.7%. However, the model before fine tuning still miss out a lot of default values. For this case, since default is not good, we are trying to predict as much true default as possible. <br> \n",
    "\n",
    "After the fine tuning, we can see that the accuracy slightly increased, and the true default that is detected is also increased, and it only missed out 225 default cases. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
